{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Perceptron is a simple learning model for solving the binary classification problem. Let us take a look at the binary classification problem again:\n",
    "* Input space: $ {X} = \\mathbb{R}^d $\n",
    "* Output space: $ {Y} = \\{+1, -1\\} $\n",
    "* Problem statement: Given an input vector $ {x} \\in \\mathbb{R}^d $, we need to output a corresponding binary decision (yes/no). Several examples are:\n",
    "    * Credit card approval: the bank consultant decides whether to approve a credit card application based on the client's input information.\n",
    "    * Spam messages detection: a detector needs to classify between spam and non-spam messages, based on their contents.\n",
    "\n",
    "# Perceptron learning model\n",
    "The functional form $ h(x) $ of the perceptron gives different weights to the coordinates of the input vector. In the credit card application example, coordinates of the input vector $ x \\in \\mathbb{R}^d $ correspond to salary, debt, historical banking records, and other data of the applicant. The weights of the perceptron represent the importance of each individual coordinate in deciding whether to approve the application. The perceptron computes the weighted sum of all coordinates, then compares to a threshold value:\n",
    "\n",
    "YES if $ \\sum_{i=1}^{d} w_ix_i > $ threshold\n",
    "\n",
    "NO if $ \\sum_{i=1}^{d} w_ix_i < $ threshold\n",
    "\n",
    "In other words:\n",
    "\n",
    "$ h(x) = sign((\\sum_{i=1}^{d} w_ix_i) + b) $\n",
    "\n",
    "where $ sign(s) = +1 $ if $ s > 0 $ and $ sign(s) = -1 $ if $ s < 0 $, and $ b $ is the bias value that corresponds to the threshold.\n",
    "\n",
    "To simplify the functional form of the perceptron, we introduce an extra value $ w_0 = b $ and merge it with the other weights. The input vector is also appended with one extra value $ x_0 = 1 $ at the beginning. The perceptron's formula can now be rewritten as:\n",
    "<br><br>\n",
    "$ h(x) = sign(w^Tx) $\n",
    "\n",
    "# Perceptron learning algorithm\n",
    "Suppose the data set is linearly separable, the perceptron learning algorithm guarantees to find the optimal weights that can correctly classify all training examples. Let $ w(t) $ be the weights vector of the perceptron at iteration $t$, where $t = $ 0, 1, 2,.... At iteration t, the algorithm randomly picks a misclassified example $ (x_i, y_i) $ and uses it to update $ w(t) $ according to the following rule:\n",
    "<br><br>\n",
    "$ w(t+1) = w(t) + y(t)x(t) $\n",
    "\n",
    "The update rule only considers one failed example at a time, and it might mess up other \"already correct\" examples after the current iteration. However, it guarantees to find the correct weights to classify all training examples. The update rule can be proved as follows:\n",
    "\n",
    "### Proof 1 (with picture)\n",
    "\n",
    "### Proof 2\n",
    "Let $ (x(t), y(t)) $ be the chosen misclassified example, and $ w(t) $ be the weights value in iteration $t$. We have:\n",
    "* $ w^T(t)x(t) $: weighted sum of the current weights vector and the input vector $ x(t) $.\n",
    "* As $ (x(t), y(t)) $ is a misclassified example: $ y(t)w^T(t)x(t) < 0 $\n",
    "\n",
    "We need to update the weights vector so that at some iteration $ t+k $, we get $ y(t)w^T(t+k)x(t) > 0 $\n",
    "\n",
    "By updating $ w(t+1) = w(t) + y(t)x(t) $, we have:\n",
    "\n",
    "$ y(t)w^T(t+1)x(t) = y(t)(w(t) + y(t)x(t))^Tx(t) = y(t)w^T(t)x(t) + y^2(t)x^T(t)x(t) $\n",
    "\n",
    "Because $ y^2(t)x^T(t)x(t) > 0 $, we got $ y(t)w^T(t+1)x(t) > y(t)w^T(t)x(t) $, which makes it more likely to be larger than 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "This is an example of the perceptron learning algorithm. We are given a set of N 2D points, which are classified as red and blue points and they are linearly separable. Our job is to find a line that can correctly separate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the $ sign() $ function that is mentioned above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sign(val):\n",
    "    if abs(val) < 0.000000001:\n",
    "        return 0\n",
    "    if val > 0:\n",
    "        return 1\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define class Point and Line to represent the training examples and target function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Point:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def asVector(self):\n",
    "        return [self.x, self.y]\n",
    "    \n",
    "    @staticmethod\n",
    "    def random():\n",
    "        return Point(random.uniform(-1, 1), random.uniform(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Line:\n",
    "    def __init__(self, a, b, c):\n",
    "        # ax + by + c = 0\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.c = c\n",
    "    \n",
    "    # return the point on this line with horizontal coordinate x\n",
    "    def getPointAtX(self, x):\n",
    "        if self.b == 0:\n",
    "            return Point(x, -1)\n",
    "        return Point(x, (-self.a*x - self.c) / self.b)\n",
    "    \n",
    "    # evaluate whether a given point is on the upper or lower area of this line\n",
    "    def evaluatePoint(self, point):\n",
    "        return sign(self.a*point.x + self.b*point.y + self.c)\n",
    "    \n",
    "    # generate a random line equation (equivalent to generating a random target function in our example)\n",
    "    @staticmethod\n",
    "    def random():\n",
    "        # generate two random points\n",
    "        p1 = Point.random()\n",
    "        p2 = Point.random()\n",
    "        \n",
    "        # compute the perpendicular vector of this line\n",
    "        u = [-(p2.y - p1.y), p2.x - p1.x]\n",
    "        return Line(u[0], u[1], -u[0]*p1.x - u[1]*p1.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Perceptron learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, dimensions):\n",
    "        self.dimensions = dimensions\n",
    "        \n",
    "        # the weights vector is appended with one extra element for the bias term\n",
    "        self.weights = [0 for _ in range(dimensions + 1)]\n",
    "    \n",
    "    # evaluate the weighted sum of the current weights vector and the input vector\n",
    "    def evaluate(self, inputs):\n",
    "        if (len(inputs) != self.dimensions):\n",
    "            raise Exception()\n",
    "        return sum([val * weight for (val, weight) in zip([1] + inputs, self.weights)])\n",
    "    \n",
    "    # classify the input point\n",
    "    def classify(self, point):\n",
    "        return sign(self.evaluate(point.asVector()))\n",
    "    \n",
    "    # update the weights vector\n",
    "    def learn(self, point, label):\n",
    "        self.weights = [w + label*x for (w, x) in zip(self.weights, [1] + point.asVector())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def experiment(N, find_difference_fg=False, plot=False):\n",
    "    # initialize target function\n",
    "    L = Line.random()\n",
    "\n",
    "    # initialize N random points\n",
    "    # X0 and Y0 store the X and Y coordinates of points with label 1\n",
    "    # X1 and Y1 store the X and Y coordinates of points with label -1\n",
    "    training_examples = [Point.random() for _ in range(N)]\n",
    "    X0 = [training_examples[_].x for _ in range(N) if L.evaluatePoint(training_examples[_]) == 1]\n",
    "    Y0 = [training_examples[_].y for _ in range(N) if L.evaluatePoint(training_examples[_]) == 1]\n",
    "    X1 = [training_examples[_].x for _ in range(N) if L.evaluatePoint(training_examples[_]) == -1]\n",
    "    Y1 = [training_examples[_].y for _ in range(N) if L.evaluatePoint(training_examples[_]) == -1]\n",
    "\n",
    "    # initialize perceptron with 2 dimensions (2D points, so 2 dimensions)\n",
    "    perceptron = Perceptron(2)\n",
    "\n",
    "    # find the misclassified examples\n",
    "    misclassified = [point for point in training_examples if perceptron.classify(point) != L.evaluatePoint(point)]\n",
    "\n",
    "    # repeat learning while the number of misclassified examples is not yet 0\n",
    "    num_iters = 0\n",
    "    while misclassified:\n",
    "        num_iters += 1\n",
    "        # choose a random misclassified point\n",
    "        point = random.choice(misclassified)\n",
    "\n",
    "        # update new weights according to this point\n",
    "        perceptron.learn(point, L.evaluatePoint(point))\n",
    "\n",
    "        # find the misclassified again\n",
    "        misclassified = [point for point in training_examples if perceptron.classify(point) != L.evaluatePoint(point)]\n",
    "\n",
    "    # the line equation that we found\n",
    "    G = Line(perceptron.weights[1], perceptron.weights[2], perceptron.weights[0])\n",
    "    \n",
    "    if find_difference_fg:\n",
    "        count = 10000\n",
    "        examples = [Point.random() for _ in range(count)]\n",
    "        misclassified = [point for point in examples if perceptron.classify(point) != L.evaluatePoint(point)]\n",
    "        return [num_iters, len(misclassified) / count]\n",
    "    \n",
    "    if plot:\n",
    "        fig = plt.figure(figsize=(16, 5))\n",
    "        ax1 = fig.add_subplot(1, 2, 1)\n",
    "        ax2 = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "        ax1.set_xlim(-1, 1)\n",
    "        ax1.set_ylim(-1, 1)\n",
    "        ax2.set_xlim(-1, 1)\n",
    "        ax2.set_ylim(-1, 1)\n",
    "\n",
    "        ax1.scatter(X0, Y0, color=\"r\")\n",
    "        ax1.scatter(X1, Y1, color=\"b\")\n",
    "        ax1.plot((-1, 1), (L.getPointAtX(-1).y, L.getPointAtX(1).y))\n",
    "\n",
    "        ax2.scatter(X0, Y0, color=\"r\")\n",
    "        ax2.scatter(X1, Y1, color=\"b\")\n",
    "        ax2.plot((-1, 1), (L.getPointAtX(-1).y, L.getPointAtX(1).y), color=\"g\")\n",
    "        ax2.plot((-1, 1), (G.getPointAtX(-1).y, G.getPointAtX(1).y), color=\"b\")\n",
    "    \n",
    "    return num_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total iterations = 116546\n",
      "average number of iterations = 116.546000\n"
     ]
    }
   ],
   "source": [
    "# Find the average number of iterations for the perceptron to converge\n",
    "N_experiments = 1000\n",
    "N_points = 100\n",
    "total_iterations = sum([experiment(N_points) for _ in range(N_experiments)])\n",
    "print('total iterations = {:d}'.format(total_iterations))\n",
    "print('average number of iterations = {:f}'.format(total_iterations / N_experiments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average difference probability = 0.012972\n"
     ]
    }
   ],
   "source": [
    "# For N = 10, compute P[f(x) != g(x)], in which f(x) is the target function and g(x) is our optimal function\n",
    "# found by our perceptron. This can be calculated by generating a large sample of random points and counting\n",
    "# the number of disagreements between these 2 functions\n",
    "N_experiments = 50\n",
    "N_points = 100\n",
    "average_difference = sum([experiment(N_points, find_difference_fg=True)[1] for _ in range(N_experiments)]) / N_experiments\n",
    "print('average difference probability = {:f}'.format(average_difference))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
