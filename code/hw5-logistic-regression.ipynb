{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "This code is implemented to answer question 8 & 9 in homework 5 of the Learning from Data course at Caltech: http://work.caltech.edu/homework/hw5.pdf\n",
    "\n",
    "In this problem, we learn how to use logistic regression for classification.\n",
    "\n",
    "# Problem statement\n",
    "In this problem, you will create your own target function $f$ (probability in this case) and dataset $\\mathcal{D}$ to see how logistic regression works. For simplicity, we will take $f$ to be a $0$/$1$ probability so $y$ is a deterministic function of x.\n",
    "\n",
    "Take $d = 2$ so you can visualize the problem, and let $\\mathcal{X} = [-1, 1]\\times[-1, 1]$ with uniform probability of picking each $x \\in \\mathcal{X}$. Choose a line in the plane as the boundary between $f(x) = 1$ (where $y$ has to be $+1$) and $f(x) = 0$ (where $y$ has to be $-1$) by taking two random, uniformly distributed points from $\\mathcal{X}$ and taking the line passing through them as the boundary between $y = \\pm 1$. Pick $N = 100$ training points at random from $\\mathcal{X}$, and evaluate the outputs $y_n$ for each of these points $\\mathbf{x}_n$.\n",
    "\n",
    "Run logistic regression with stochastic gradient descent to find the best parameters for our model, and estimate $E_{out}$ (the cross entropy error) by generating a sufficiently large, separate set of points to evaluate the error. Repeat the experiment for $100$ runs with different targets and take the average. Initialize the weight vector of logistic regression to all zeros in each run. Stop the algorithm when $\\| \\mathbf{w}^{(t-1)} - \\mathbf{w}^{(t)} \\| < 0.01 $, where $\\mathbf{w}^{(t)}$ denotes the weight vector at the end of epoch $t$. An epoch is a full pass through $N$ data points (use a random permutation of $1, 2, ..., N$ to present the data points to the algorithm within each epoch, and use different permutations for different epochs). Use a learning rate of $0.01$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate sigmoid function.\n",
    "def sigmoid(s):\n",
    "    return math.e**s / (1 + math.e**s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class Point 2D\n",
    "class Point:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def asVector(self):\n",
    "        return np.array([self.x, self.y])\n",
    "    \n",
    "    @staticmethod\n",
    "    def random():\n",
    "        return Point(random.uniform(-1, 1), random.uniform(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class Line in the 2D plane\n",
    "class Line:\n",
    "    def __init__(self, a, b, c):\n",
    "        # ax + by + c = 0\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.c = c\n",
    "    \n",
    "    # return the point on this line with horizontal coordinate x\n",
    "    def getPointAtX(self, x):\n",
    "        if self.b == 0:\n",
    "            return Point(x, -1)\n",
    "        return Point(x, (-self.a*x - self.c) / self.b)\n",
    "    \n",
    "    # evaluate whether a given point is on the upper or lower area of this line\n",
    "    def evaluatePoint(self, point):\n",
    "        return np.sign(self.a*point.x + self.b*point.y + self.c)\n",
    "    \n",
    "    # generate a random line equation (equivalent to generating a random target function in our example)\n",
    "    @staticmethod\n",
    "    def random():\n",
    "        # generate two random points\n",
    "        p1 = Point.random()\n",
    "        p2 = Point.random()\n",
    "        \n",
    "        # compute the perpendicular vector of this line\n",
    "        u = [-(p2.y - p1.y), p2.x - p1.x]\n",
    "        return Line(u[0], u[1], -u[0]*p1.x - u[1]*p1.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    @staticmethod\n",
    "    def train(X, Y, learning_rate=0.01):\n",
    "        # Get the number of data points and the dimensionality of the input space.\n",
    "        N, d = X.shape\n",
    "        # Add the bias term to the vector of each data point.\n",
    "        x = np.array([np.insert(X[i], 0, 1) for i in range(N)])\n",
    "        \n",
    "        # Initial weights.\n",
    "        W = np.zeros(d + 1)\n",
    "        \n",
    "        # Weight in the previous epoch. This is to compare the squared difference between the weight vectors\n",
    "        # of 2 consecutive epochs and determine whether to terminate the algorithm.\n",
    "        prev_W = np.zeros(d + 1)\n",
    "        \n",
    "        n_iters = 0\n",
    "        # Repeat until convergence.\n",
    "        while True:\n",
    "            n_iters += 1\n",
    "            # Randomly generate a permutation of N numbers [0, 1, ..., N-1].\n",
    "            # Each number is used as an index to pick a random data point from the dataset and put into our\n",
    "            # stochastic gradient descent.\n",
    "            P = np.random.permutation(N)\n",
    "            \n",
    "            for idx in P:\n",
    "                # Compute the partial derivative of the error function with respect to the weight vector at\n",
    "                # the current values of W.\n",
    "                dE = -Y[idx] * x[idx] * sigmoid(-Y[idx] * np.dot(W, x[idx]))\n",
    "                \n",
    "                # Move down the hill.\n",
    "                W -= learning_rate * dE\n",
    "                \n",
    "            # Compute the cross entropy error for the training dataset.\n",
    "            # error = LogisticRegression.getError(W, x, Y)\n",
    "            # print(error)\n",
    "\n",
    "            # Compute the squared difference between the previous and the current weight vector.\n",
    "            diff = np.dot(prev_W - W, prev_W - W) ** 0.5\n",
    "            # print('diff = ', diff)\n",
    "\n",
    "            # Terminate if difference is less than 0.01\n",
    "            if diff < 0.01:\n",
    "                break\n",
    "            prev_W = np.copy(W)\n",
    "        \n",
    "        return [W, n_iters]\n",
    "    \n",
    "    def getError(W, x, Y):\n",
    "        res = 0\n",
    "        for i in range(x.shape[0]):\n",
    "            res += math.log(1 / sigmoid(Y[i] * np.dot(W, x[i])))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perform experiment with N data points.\n",
    "def experiment(N, plot=False):\n",
    "    # N is the number of data points to generate.\n",
    "\n",
    "    # Initialize a random line to separate the plane into 2 parts: one for blue points, one for red points.\n",
    "    L = Line.random()\n",
    "\n",
    "    # Initialize N random points.\n",
    "    # X0 and Y0 store the X and Y coordinates of points with label 1.\n",
    "    # X1 and Y1 store the X and Y coordinates of points with label -1.\n",
    "    training_examples = [Point.random() for _ in range(N)]\n",
    "    X0 = [training_examples[_].x for _ in range(N) if L.evaluatePoint(training_examples[_]) == 1]\n",
    "    Y0 = [training_examples[_].y for _ in range(N) if L.evaluatePoint(training_examples[_]) == 1]\n",
    "    X1 = [training_examples[_].x for _ in range(N) if L.evaluatePoint(training_examples[_]) == -1]\n",
    "    Y1 = [training_examples[_].y for _ in range(N) if L.evaluatePoint(training_examples[_]) == -1]\n",
    "\n",
    "    # Create data to put into logistic regression class for learning.\n",
    "    X = np.array([training_examples[_].asVector() for _ in range(N)])\n",
    "    Y = np.array([L.evaluatePoint(training_examples[_]) for _ in range(N)])\n",
    "\n",
    "    # Find the best parameters.\n",
    "    [W, n_iters] = LogisticRegression.train(X, Y)\n",
    "    \n",
    "    # Classify using our computed parameters.\n",
    "    X2 = [training_examples[_].x for _ in range(N) if \\\n",
    "          sigmoid(np.dot(W, np.insert(training_examples[_].asVector(), 0, 1))) >= 0.5]\n",
    "    Y2 = [training_examples[_].y for _ in range(N) if \\\n",
    "          sigmoid(np.dot(W, np.insert(training_examples[_].asVector(), 0, 1))) >= 0.5]\n",
    "    X3 = [training_examples[_].x for _ in range(N) if \\\n",
    "          sigmoid(np.dot(W, np.insert(training_examples[_].asVector(), 0, 1))) < 0.5]\n",
    "    Y3 = [training_examples[_].y for _ in range(N) if \\\n",
    "          sigmoid(np.dot(W, np.insert(training_examples[_].asVector(), 0, 1))) < 0.5]\n",
    "\n",
    "    if plot:\n",
    "        fig = plt.figure(figsize=(16, 5))\n",
    "        ax1 = fig.add_subplot(1, 2, 1)\n",
    "        ax2 = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "        ax1.set_xlim(-1, 1)\n",
    "        ax1.set_ylim(-1, 1)\n",
    "        ax2.set_xlim(-1, 1)\n",
    "        ax2.set_ylim(-1, 1)\n",
    "\n",
    "        ax1.scatter(X0, Y0, color=\"r\")\n",
    "        ax1.scatter(X1, Y1, color=\"b\")\n",
    "        ax1.plot((-1, 1), (L.getPointAtX(-1).y, L.getPointAtX(1).y), color=\"r\")\n",
    "\n",
    "        ax2.scatter(X2, Y2, color=\"r\")\n",
    "        ax2.scatter(X3, Y3, color=\"b\")\n",
    "        \n",
    "    return [L, W, n_iters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure on the left shows N data points with the decision boundary that separates them.\n",
      "Figure on the right shows the color of those N data points that have been classified by our logistic regression model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA68AAAE4CAYAAAC0ZMU7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1063c4860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of an experiment with 100 points.\n",
    "[L, W, n_iters] = experiment(100, True)\n",
    "print('Figure on the left shows N data points with the decision boundary that separates them.')\n",
    "print('Figure on the right shows the color of those N data points that have been classified by our logistic regression model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected E_out =  0.10349230951568499\n",
      "Average number of iterations =  343.87\n"
     ]
    }
   ],
   "source": [
    "# Number of experiments.\n",
    "N_experiments = 100\n",
    "\n",
    "# Expected out-of-sample cross entropy error\n",
    "E_out = 0\n",
    "\n",
    "# Average number of iterations\n",
    "avg_iters = 0\n",
    "for _ in range(N_experiments):\n",
    "    # Number of data points to estimate E_out\n",
    "    N_train_examples = 100\n",
    "    [L, W, n_iters] = experiment(N_train_examples)\n",
    "    avg_iters += n_iters\n",
    "    \n",
    "    N_test_examples = 1000\n",
    "    test_examples = [Point.random() for _ in range(N_test_examples)]\n",
    "    X = np.array([np.insert(test_examples[_].asVector(), 0, 1) for _ in range(N_test_examples)])\n",
    "    Y = np.array([L.evaluatePoint(test_examples[_]) for _ in range(N_test_examples)])\n",
    "    E_out += LogisticRegression.getError(W, X, Y) / N_test_examples\n",
    "\n",
    "E_out /= N_experiments\n",
    "print('Expected E_out = ', E_out)\n",
    "\n",
    "avg_iters /= N_experiments\n",
    "print('Average number of iterations = ', avg_iters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
